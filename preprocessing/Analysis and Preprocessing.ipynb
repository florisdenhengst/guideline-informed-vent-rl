{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import poisson\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts/')\n",
    "from config import demographics, vital_sign_vars, lab_vars, treatment_vars, vent_vars, guideline_vars, ffill_windows_clinical, SAMPLE_TIME_H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Read file with a selection of patients according to the following criteria:\n",
    "* age >= 18 years\n",
    "* ventilated for >= 24 hours\n",
    "* length of ICU stay >= 72 hours\n",
    "* known hospital mortality or 90-day-mortality\n",
    "\n",
    "Data is assumed to be sampled and/or aggregated by periods of 4 hours for the entire ICU stay, i.e. including after and prior to mechanical ventilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../mimic-data/ventilatedpatients.csv'\n",
    "seed = 12\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.subject_id.nunique(), df.icustay_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing phase 1\n",
    "Set some time stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = [\n",
    "    'start_time',\n",
    "    'intime',\n",
    "    'outtime',\n",
    "]\n",
    "for col in timestamps:\n",
    "    df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "df = df.sort_values(['icustay_id', 'start_time']).reset_index().drop(['index', ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_fill_between(df, colname, fill_value):\n",
    "    def my_fill_between(group, colname, fill_value):\n",
    "        group = group.sort_values('start_time')\n",
    "        from_i = group[colname].first_valid_index()\n",
    "        to_i = group[colname].last_valid_index()\n",
    "        return group.loc[from_i:to_i][colname].fillna(fill_value)\n",
    "    grouped = df.sort_values(['start_time', 'icustay_id']).groupby('icustay_id')\n",
    "    grouped_filled = grouped.apply(lambda x: my_fill_between(x, colname, fill_value))\n",
    "    return grouped_filled.reset_index().set_index('level_1')[colname]\n",
    "\n",
    "df['vent_num_imp'] = grouped_fill_between(df, 'vent_num', 1.0)\n",
    "df['vent_imp_step'] = df.groupby('icustay_id').vent_num_imp.cumsum()\n",
    "df['vent_shift'] = df.vent_num_imp.shift(-1)\n",
    "\n",
    "df['select_peine'] = (~df['vent_shift'].isna() | (df['vent_num_imp'] == 1.0 )) & (df['vent_imp_step'] <= 18.0)\n",
    "df['select_peine_step'] = df.groupby('icustay_id').select_peine.cumsum()\n",
    "df['select_peine_step_rev'] = df.sort_values(['icustay_id', 'start_time'], ascending=False).groupby('icustay_id').select_peine.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['vent_num_imp', 'vent_num', 'select_peine', 'select_peine_step', 'select_peine_step_rev', 'start_time', 'icustay_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the admission time for each icustay as ``start_time_icustay``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df.groupby('icustay_id', as_index=False)['start_time'].min(), on='icustay_id', how='left').rename(\n",
    "    columns={'start_time_x': 'start_time', 'start_time_y': 'start_time_icustay'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate length of stay at the ICU: LOS ICU and length of stay in hospital: LOS Hosp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of stay at ICU\n",
    "df['los_icu'] = df.outtime - df.intime\n",
    "# length of stay at hospital, LOS\n",
    "df.loc[:, 'hadmittime'] = pd.to_datetime(df.hadmittime)\n",
    "df.loc[:, 'hdischtime'] = pd.to_datetime(df.hdischtime)\n",
    "df.loc[:, 'hadm_duration'] = df.hdischtime - df.hadmittime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'mort'] = (df.mort90day == 't') | (df.hospmort == 't')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('icustay_id').first().mort.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.groupby('icustay_id').first().mort90day == 't').mean(), (df.groupby('icustay_id').first().hospmort == 't').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_missing_pct(dataframe, variables):\n",
    "    \"\"\"\n",
    "    Formatted print of the number of NaNs in `dataframe` for each variable in `variables`.\n",
    "    \"\"\"\n",
    "    ljust = max(map(len, variables))\n",
    "    for var in variables:\n",
    "        print('{} {}'.format(var.ljust(ljust), dataframe[var].isna().mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographics\n",
    "Analyze the missing data per patient for demographics:\n",
    "* Age\n",
    "* Gender\n",
    "* Weight (upon admission)\n",
    "* ICU readmission\n",
    "* Premorbidity as calculated by exlixhauser vanWalraven score by previous hospital stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that demographics do not change over time, so we can take any value. We take the first.\n",
    "identifiers = ['icustay_id',]\n",
    "\n",
    "patients = df.drop_duplicates(identifiers, keep='first')\n",
    "print_missing_pct(patients, demographics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vital signs\n",
    "Analyze missing data across the entire dataset for vital signs. These can be composed of both lab and chart data:\n",
    "* SOFA: sequential organ failure, see [code](https://github.com/MIT-LCP/mimic-code/blob/master/concepts/severityscores/sofa.sql)\n",
    "* SIRS: systemic inflammatory response syndrome (SIRS) score, see [code](https://github.com/MIT-LCP/mimic-code/blob/master/concepts/severityscores/sirs.sql)\n",
    "* GCS: glascow coma score, see [code](https://github.com/MIT-LCP/mimic-code/blob/master/concepts/pivot/pivoted-gcs.sql)\n",
    "* HR: heartrate, averaged over period\n",
    "* SysBP: systolic blood pressure, averaged over period\n",
    "* MeanBP: mean arterial pressure, averaged over period\n",
    "* DiasBP: diastolic blood pressure, averaged over period\n",
    "* Shock Index: SysBP / HeartRate, averaged over period\n",
    "* RespRate: respiratory rate, averaged over period\n",
    "* SpO2: blood oxygenation saturation, averaged over period\n",
    "* TempC: temperature in Celsius, averaged over period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_missing_pct(df, vital_sign_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab measurements\n",
    "All are averaged over the time span\n",
    "* potassium\n",
    "* sodium\n",
    "* chloride\n",
    "* glucose\n",
    "* bun: blood urea nitrogen\n",
    "* creatinine\n",
    "* magnesium\n",
    "* calcium\n",
    "* ionizedcalcium\n",
    "* carbondioxide\n",
    "* bilirubin\n",
    "* albumin\n",
    "* hemoglobin\n",
    "* wbc: white blood cell count\n",
    "* platelet: platelets count\n",
    "* ptt: Partial Thromboplastin Time\n",
    "* pt: Prothrombin Time\n",
    "* inr: International Normalized Ratio\n",
    "* ph\n",
    "* pao2\n",
    "* paco2\n",
    "* base_ excess\n",
    "* bicarbonate\n",
    "* lactate\n",
    "* pao2fio2ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_missing_pct(df, lab_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_missing_pct(df, lab_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic, non-ventilation-related treatment variables\n",
    "* iv_total\n",
    "* vaso_total\n",
    "* urineoutput\n",
    "* cum_fluid_balance: cumulated fluid balance since admission, incl. preadmission when available\n",
    "\n",
    "These can be 'missing' due to not being applied (e.g. vasopressors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_missing_pct(df, treatment_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ventilation settings\n",
    "* peep\n",
    "* fio2\n",
    "* tidal_volume\n",
    "* mechvent\n",
    "\n",
    "These can be 'missing' due to mechanical ventilation not being applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_missing_pct(df, vent_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guideline vars\n",
    "Some variables are in the guideline but were not in the original feature selection:\n",
    "* plateau_pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_missing_pct(df, guideline_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency analysis per variable\n",
    "We estimate whether a new observation/measurement is expected in the following way:\n",
    "* for every given observation, we calculate the time it took to observe.\n",
    "\n",
    "  Specifically, we calculate the time from the previous observation OR admission.\n",
    "* define a distribution of time it takes to do an observation (mean/avg?)\n",
    "* estimate the expectation of a new probability based on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tukeys_fence(series, includena=True):\n",
    "    q1, q3 = series.quantile(.25), series.quantile(.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_fence, upper_fence = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "    valuefilter = (series > lower_fence) & (series < upper_fence)\n",
    "    if includena:\n",
    "        return valuefilter | series.isna()\n",
    "    else:\n",
    "        return valuefilter\n",
    "    \n",
    "\n",
    "def ceil_to_nearest_multiple(n, multiple=SAMPLE_TIME_H):\n",
    "    \"\"\"\n",
    "    Rounds `n` up to nearest `multiple`.\n",
    "    \"\"\"\n",
    "    return multiple * math.ceil(n / multiple)\n",
    "\n",
    "def get_poisson_cutoff(data, quantile=.99, label='Data', sample_time=SAMPLE_TIME_H, plot=False):\n",
    "    \"\"\"\n",
    "    Returns the value so that `quantile` % of the `data` lies below that value.\n",
    "    \"\"\"\n",
    "    data = data - 1\n",
    "    # the bins should be of integer width, because poisson is an integer distribution\n",
    "    bins = np.arange(np.nanmax(data)) - 0.5\n",
    "    entries, bin_edges = np.histogram(data, bins=bins, density=True)\n",
    "\n",
    "    # calculate bin centres\n",
    "    bin_middles = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
    "\n",
    "    def fit_function(k, lamb):\n",
    "        '''poisson function, parameter lamb is the fit parameter'''\n",
    "        return poisson.pmf(k, lamb)\n",
    "\n",
    "    # fit with curve_fit\n",
    "    parameters, cov_matrix = curve_fit(fit_function, bin_middles, entries)\n",
    "    cdf = poisson.cdf(np.arange(np.nanmax(data)), *parameters)\n",
    "    cutoff = len(cdf[cdf < quantile])\n",
    "    \n",
    "    if plot:\n",
    "        plt.hist(data, bins=bins, density=True, label=label)\n",
    "        x_plot = np.arange(0, np.nanmax(data))\n",
    "        axs = plt.plot(\n",
    "            x_plot,\n",
    "            fit_function(x_plot, *parameters),\n",
    "            marker='o', linestyle='',\n",
    "            label='poisson fit'\n",
    "        )\n",
    "        ax = plt.gca()\n",
    "        plt.xlabel('x4 hours')\n",
    "        plt.title(\"{}@{}: {}, max={}\".format(label, quantile, cutoff*sample_time,np.nanmax(data)*sample_time))\n",
    "        plt.legend()\n",
    "        plt.vlines(cutoff, 0, max(entries)*1.1, colors=['black'])\n",
    "        plt.show()\n",
    "    \n",
    "    return cutoff\n",
    "\n",
    "def observation_durations(data, colname, groupby='icustay_id', sample_time=SAMPLE_TIME_H, scope_filter=None):\n",
    "    \"\"\"\n",
    "    Returns the duration from admission or the previous measurements for each measurement of `colname` in `data`, for admissions identified by the `groupby` column and sample frequency of `sample_time`.\n",
    "    \"\"\"\n",
    "    if scope_filter is None:\n",
    "        scope_filter = pd.Series([True,] * data.shape[0])\n",
    "    # initialize a copy of the `data` dataframe with some placeholder columns\n",
    "    temp_df = pd.DataFrame.from_dict({\n",
    "        'icustay_id': data.icustay_id,\n",
    "        'timestamp': data.start_time,\n",
    "        'timestamp_icustay': data.start_time_icustay,\n",
    "        colname: data[colname],\n",
    "        't_start': np.nan,\n",
    "        'ages': pd.NaT,\n",
    "    })\n",
    "    # first set icu admission times as start time\n",
    "    temp_df.loc[temp_df.timestamp == temp_df.timestamp_icustay, 't_start'] = temp_df[temp_df.timestamp == temp_df.timestamp_icustay].timestamp\n",
    "    # then add timings of observations of variable in `colname`\n",
    "    temp_df.loc[~temp_df[colname].isna(), 't_start'] = temp_df[~temp_df[colname].isna()].timestamp\n",
    "    # now forward fill per icustay\n",
    "    temp_df['t_start'] = temp_df.groupby(groupby)['t_start'].ffill()\n",
    "    # and shift back by 1 time step per icustay\n",
    "    temp_df['t_start'] = temp_df.groupby(groupby)['t_start'].shift()\n",
    "    # and calculate the time from [previous observation|admission] until this observation\n",
    "    temp_df.loc[~temp_df[colname].isna(), 'duration'] = pd.to_datetime(temp_df.timestamp) - pd.to_datetime(temp_df.t_start)\n",
    "    # set time of first measurement to zero\n",
    "    temp_df.loc[(temp_df['timestamp'] == temp_df['timestamp_icustay']) & ~temp_df[colname].isna()] = pd.Timedelta(0, 'hours')\n",
    "    # add the length of time window to duration\n",
    "    return temp_df[(~data[colname].isna() & scope_filter)]['duration'] + pd.Timedelta(sample_time, 'hours')\n",
    "\n",
    "def observation_durations_ventstart(data, colname, groupby='icustay_id', sample_time=SAMPLE_TIME_H, scope_filter=None):\n",
    "    \"\"\"\n",
    "    Returns the duration from admission or the previous measurements for each measurement of `colname` in `data`, for admissions identified by the `groupby` column and sample frequency of `sample_time`.\n",
    "    \"\"\"\n",
    "    if scope_filter is None:\n",
    "        scope_filter = pd.Series([True,] * data.shape[0])\n",
    "    # initialize a copy of the `data` dataframe with some placeholder columns\n",
    "    temp_df = pd.DataFrame.from_dict({\n",
    "        'icustay_id': data.icustay_id,\n",
    "        'timestamp': data.start_time,\n",
    "        'timestamp_icustay': data.start_time_icustay,\n",
    "        colname: data[colname],\n",
    "        't_start': np.nan,\n",
    "        'ages': pd.NaT,\n",
    "        'mechvent_start_peine': (data.mechvent_start_peine == 1.0).fillna(False),\n",
    "    })\n",
    "\n",
    "    # first add timings of observations of variable in `colname`\n",
    "    temp_df.loc[~temp_df[colname].isna(), 't_start'] = temp_df[~temp_df[colname].isna()].timestamp\n",
    "    # forward fill, first pass\n",
    "    temp_df.t_start.ffill()\n",
    "    # then set the ventilation start as start time, if it is still missing\n",
    "    temp_df.loc[temp_df.t_start.isna() & (temp_df.mechvent_start_peine), 't_start'] = temp_df[temp_df.t_start.isna() & (temp_df.mechvent_start_peine)].timestamp\n",
    "    # now forward fill per icustay\n",
    "    temp_df['t_start'] = temp_df.groupby(groupby)['t_start'].ffill()\n",
    "    # and shift back by 1 time step per icustay\n",
    "    temp_df['t_start'] = temp_df.groupby(groupby)['t_start'].shift()\n",
    "    # and calculate the time from [previous observation|admission] until this observation\n",
    "    temp_df.loc[~temp_df[colname].isna(), 'duration'] = pd.to_datetime(temp_df.timestamp) - pd.to_datetime(temp_df.t_start)\n",
    "    # set time of first measurement to zero\n",
    "    temp_df.loc[(temp_df.mechvent_start_peine) & ~temp_df[colname].isna()] = pd.Timedelta(0, 'hours')\n",
    "    # add the length of time window to duration\n",
    "    return temp_df[(~data[colname].isna() & scope_filter)]['duration'] + pd.Timedelta(sample_time, 'hours')\n",
    "\n",
    "def get_cutoff_duration(data, colname, groupby='icustay_id', quantile=.99, sample_time=SAMPLE_TIME_H, plot=False, scope_filter=None):\n",
    "    \"\"\"\n",
    "    Returns the duration so that `quantile` % of the `colname` measurements in `data` where obtained within that duration.\n",
    "    \"\"\"\n",
    "    # obtain durations\n",
    "#     durations = observation_durations(data, colname, groupby, scope_filter=scope_filter)\n",
    "    durations = observation_durations_ventstart(data, colname, groupby, scope_filter=scope_filter)\n",
    "    # convert to numpy array of hours\n",
    "    duration_hours = (durations.dt.components.days * 24 + durations.dt.components.hours).to_numpy()\n",
    "    # outlier removal using Tukey's fence\n",
    "    durations = durations[tukeys_fence(durations)]\n",
    "    return get_poisson_cutoff(duration_hours / sample_time, quantile, colname, plot=True) * sample_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demographics\n",
    "Some pre-selection demographics inspired by Table 1 in Peine et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique identifier columns\n",
    "identifiers = ['icustay_id', 'subject_id', 'hadm_id',]\n",
    "# columns with patient demographics information\n",
    "demographics = ['admission_age', 'adult_ibw', 'height', 'weight', 'icu_readmission', 'elixhauser_vanwalraven', 'hospmort', 'mort90day', 'vent_duration_h']\n",
    "patients = df.drop_duplicates(identifiers + demographics, keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients.icustay_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = len(demographics)\n",
    "plt.subplots(1, len(demographics), figsize=(50,5))\n",
    "for i in range(n_cols):\n",
    "    plt.subplot(1, n_cols, i+1)\n",
    "    if is_numeric_dtype(patients[demographics[i]]):\n",
    "        sns.boxplot(data=patients, x='gender', y=demographics[i], orient='v')\n",
    "    else:\n",
    "        sns.countplot(data=patients, x=demographics[i], orient='v', hue='gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mechvent events\n",
    "Create a single mechanical ventilation event per icustay.\n",
    "Peine et. al simply sample the first recorded mechvent event.\n",
    "- for each patient:\n",
    "   - count each consecutive mechvent using mechvent_start_peine\n",
    "   - assign this count label to each timestep at which mechvent_peine is true\n",
    "   - calculate timestep before and 18 (=72/4) timesteps after onset of mechanical ventilation\n",
    "   - filter by these calculated timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation\n",
    "Steps according to Peine et al.\n",
    "* Time-windowed sample-and-hold e.g. forward-fill.\n",
    "\n",
    "  Windows sizes were determined using a frequency analysis of the dataset, producing the frequency of production of a new datapoint (per variable)\n",
    "* K-nearest neighbor imputation with imputation and SVD\n",
    "* If >50% of missing data, discard (<1% of cohort)\n",
    "* Test correlation between data and probability distribution of missing values for each of the 44 features.\n",
    "  Here, GCS was associated with highest p-value of 0.08 -> hence distinguish missing-at-random from missing-completely-at-random and not-missed-at-random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine sample-and-hold window with frequency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ffill_window(df, var, plot, scope_filter):\n",
    "#     val = get_cutoff_duration(df, var, quantile=.99, plot=plot, scope_filter=scope_filter)\n",
    "#     if not plot:\n",
    "#         print('.', end='') # simple measure of progress\n",
    "#     return val\n",
    "\n",
    "all_var_types = [\n",
    "    vital_sign_vars,\n",
    "    lab_vars,\n",
    "    treatment_vars,\n",
    "    vent_vars,\n",
    "    guideline_vars,\n",
    "]\n",
    "all_vars = functools.reduce(operator.add, all_var_types)\n",
    "\n",
    "# # NOTE: embarrasingly parallel, can be sped up with e.g. joblib.Parallel\n",
    "# ffill_windows_statistical = {var: ffill_window(df, var, True, df['select_peine']) for var in all_vars}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clinically informed sample-and-hold windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf = None\n",
    "ffill_windows_clinical = {\n",
    "    'sofa': 24 / SAMPLE_TIME_H,\n",
    "    'sirs': 24 / SAMPLE_TIME_H,\n",
    "    'gcs': inf,\n",
    "    'heartrate': inf,\n",
    "    'sysbp': inf,\n",
    "    'meanbp': inf,\n",
    "    'diasbp': inf,\n",
    "    'shockindex': inf,\n",
    "    'resprate': inf,\n",
    "    'spo2': inf,\n",
    "    'tempc': inf,\n",
    "    'potassium': inf,\n",
    "    'sodium': inf,\n",
    "    'chloride': inf,\n",
    "    'glucose': inf,\n",
    "    'bun': inf,\n",
    "    'creatinine': inf,\n",
    "    'magnesium': inf,\n",
    "    'calcium': inf,\n",
    "    'ionizedcalcium': 8 / SAMPLE_TIME_H,\n",
    "    'carbondioxide': inf,\n",
    "    'bilirubin': inf,\n",
    "    'albumin': inf,\n",
    "    'hemoglobin': inf,\n",
    "    'wbc': inf,\n",
    "    'platelet': inf,\n",
    "    'ptt': inf,\n",
    "    'pt': inf,\n",
    "    'inr': inf,\n",
    "    'ph': inf,\n",
    "    'pao2': inf,\n",
    "    'paco2': inf,\n",
    "    'base_excess': inf,\n",
    "    'bicarbonate': inf,\n",
    "    'lactate': inf,\n",
    "    'pao2fio2ratio': inf,\n",
    "    'iv_total': 8 / SAMPLE_TIME_H,\n",
    "    'vaso_total': 24 / SAMPLE_TIME_H,\n",
    "    'urineoutput': 8 / SAMPLE_TIME_H,\n",
    "    'cum_fluid_balance': 8 / SAMPLE_TIME_H,\n",
    "    'peep': 8 / SAMPLE_TIME_H,\n",
    "    'fio2': 8 / SAMPLE_TIME_H,\n",
    "    'tidal_volume': 8 / SAMPLE_TIME_H,\n",
    "    'mechvent': 8 / SAMPLE_TIME_H,\n",
    "    'plateau_pressure': 8 / SAMPLE_TIME_H\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute by sample-and-hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in all_vars:\n",
    "    df[var + '_imp'] = df.groupby('icustay_id')[var].ffill(limit=ffill_windows_clinical[var])\n",
    "\n",
    "imputed_vars = list(map(lambda x: x + '_imp', all_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_varname_len = max(map(len, all_vars))\n",
    "print(\"{}: {}  {}  {}\".format('VARNAME'.ljust(max_varname_len), 'MISSING', 'ORIGINAL', 'IMPUTED'))\n",
    "for var in all_vars:\n",
    "    missing = df[var].isna().sum() / df.shape[0]\n",
    "    missing_imp = df[var + '_imp'].isna().sum() / df.shape[0]\n",
    "    imputed = missing - missing_imp\n",
    "    print(\"{}: {:.4f}   {:.4f}    {:.4f}\".format(\n",
    "        var.ljust(max_varname_len),\n",
    "        missing_imp,\n",
    "        missing,\n",
    "        imputed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing values selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = df[df['select_peine']]\n",
    "\n",
    "max_varname_len = max(map(len, all_vars))\n",
    "print(\"{}: {}  {}  {}\".format('VARNAME'.ljust(max_varname_len), 'MISSING', 'ORIGINAL', 'IMPUTED'))\n",
    "for var in all_vars:\n",
    "    missing = selection[var].isna().sum() / selection.shape[0]\n",
    "    missing_imp = selection[var + '_imp'].isna().sum() / selection.shape[0]\n",
    "    imputed = missing - missing_imp\n",
    "    print(\"{}: {:.4f}   {:.4f}    {:.4f}\".format(\n",
    "        var.ljust(max_varname_len),\n",
    "        missing_imp,\n",
    "        missing,\n",
    "        imputed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier removal\n",
    "\"Outliers were sorted out with univariate statistical approaches (Tukey's range test) and frequency analysis (90% CI)\" according to Peine. Since Tukey's range test is a test for significantly different means between groups, they probably refer to Tukey's fences for outlier detection, which are defines 'non outlier' data as everything in the range ``[(first quartile - 1.5 IQR), (third quartile + 1.5 IQR)]``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_numeric_vars = {'mechvent'}\n",
    "tukeys_fences = []\n",
    "for var in all_vars:\n",
    "    if pd.api.types.is_numeric_dtype(selection[var]) and var not in boolean_numeric_vars:\n",
    "        selection.loc[:, var + '_in_tukeys_fence'] = tukeys_fence(selection[var])\n",
    "        tukeys_fences += [var + '_in_tukeys_fence',]\n",
    "        selection.loc[~selection[var + '_in_tukeys_fence'], var] = np.NaN\n",
    "        selection.loc[~selection[var + '_in_tukeys_fence'], var + '_imp'] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier removal for demographic data\n",
    "ranges = {\n",
    "    'weight': (25, 400),\n",
    "    'admission_age': (18, 150),\n",
    "    'adult_ibw': (25, 400),\n",
    "}\n",
    "for var in ranges:\n",
    "    lower, upper = ranges[var]\n",
    "    selection.loc[(selection[var] < lower) | (selection[var] > upper), var] = np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove patients with too many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = ((selection[imputed_vars + demographics].isnull().groupby(df.icustay_id).mean().mean(axis=1)) > .5)\n",
    "print(\"Removing {} / {} = {}% patients due to >50% missing values\".format(to_remove.sum(), selection.shape[0], 100*to_remove.sum()/selection.shape[0]))\n",
    "selection = selection[~selection.icustay_id.isin(to_remove[to_remove].index)]\n",
    "selected_patients = selection.drop_duplicates(subset='icustay_id', keep='first')\n",
    "\n",
    "print(\"Selection of {} timesteps for {} patients / mech vent events\".format(selection.shape[0], selected_patients.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection.mechvent.isna().sum() / selection.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test-validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-test-validation split\n",
    "* Train: 60%\n",
    "* Test: 20%\n",
    "* Validation: 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, validation = .6, .2, .2\n",
    "assert (train + test + validation) == 1.0\n",
    "\n",
    "train_split = train / (train + test)\n",
    "test_split = test / (train + test)\n",
    "\n",
    "precision = 1e8\n",
    "assert (train_split + test_split) - 1.0 < precision\n",
    "assert (train_split * (train + test)) - train < precision\n",
    "assert (test_split * (train + test)) - test < precision\n",
    "\n",
    "train_and_test_patients, validation_patients = train_test_split(selected_patients, test_size=0.2)\n",
    "train_patients, test_patients = train_test_split(train_and_test_patients, test_size=test_split)\n",
    "\n",
    "precision = 1.5\n",
    "assert abs(train_patients.shape[0] - train * selected_patients.shape[0]) < precision\n",
    "assert abs(validation_patients.shape[0] - validation * selected_patients.shape[0]) < precision\n",
    "assert abs(test_patients.shape[0] - test * selected_patients.shape[0]) < precision\n",
    "\n",
    "train_df = selection[selection.icustay_id.isin(train_patients.icustay_id)].copy()\n",
    "test_df = selection[selection.icustay_id.isin(test_patients.icustay_id)].copy()\n",
    "val_df = selection[selection.icustay_id.isin(validation_patients.icustay_id)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale & center for imputation w/ kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for k-means, data has to be scaled first.\n",
    "k-Means imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: mirror changes here in the unscaling logic in e.g. the Clustering & Analysis notebook\n",
    "numeric_scaler = StandardScaler\n",
    "\n",
    "df_stays = df.drop_duplicates(subset='icustay_id', keep='first')\n",
    "scalers = {}\n",
    "\n",
    "scaled_vars = []\n",
    "for var in all_vars:\n",
    "    if pd.api.types.is_numeric_dtype(selection[var]):\n",
    "        ffilled_varname = var + '_imp'\n",
    "        scaled_vars.append(ffilled_varname)\n",
    "\n",
    "all_var_scaler = numeric_scaler()\n",
    "scaled = all_var_scaler.fit_transform(train_df[scaled_vars])\n",
    "scaled_test = all_var_scaler.transform(test_df[scaled_vars])\n",
    "scaled_val = all_var_scaler.transform(val_df[scaled_vars])\n",
    "scaled_varnames = [var + '_scaled' for var in all_var_scaler.get_feature_names_out()]\n",
    "train_df[scaled_varnames] = scaled\n",
    "test_df[scaled_varnames] = scaled_test\n",
    "val_df[scaled_varnames] = scaled_val\n",
    "\n",
    "demographics_scaled_vars = []\n",
    "for var in demographics:\n",
    "    if pd.api.types.is_numeric_dtype(train_df[var]):\n",
    "        scaled_vars.append(var)\n",
    "        demographics_scaled_vars.append(var)\n",
    "        \n",
    "demographics_scaler = numeric_scaler()\n",
    "demographics_scaler.fit(df_stays[demographics_scaled_vars])\n",
    "demo_scaled_varnames = [var + '_scaled' for var in demographics_scaler.get_feature_names_out()]\n",
    "scaled = demographics_scaler.transform(train_df[demographics_scaled_vars])\n",
    "scaled_test = demographics_scaler.transform(test_df[demographics_scaled_vars])\n",
    "scaled_val = demographics_scaler.transform(val_df[demographics_scaled_vars])\n",
    "train_df[demo_scaled_varnames] = scaled\n",
    "test_df[demo_scaled_varnames] = scaled_test\n",
    "val_df[demo_scaled_varnames] = scaled_val\n",
    "\n",
    "joblib.dump(scaled_varnames, 'scalers/train/clinical_std_varnames_{}.bin'.format(seed), compress=True)\n",
    "joblib.dump(all_var_scaler, \"scalers/train/clinical_std_scaler_{}.bin\".format(seed), compress=True)\n",
    "joblib.dump(demo_scaled_varnames, 'scalers/train/demographics_std_varnames_{}.bin'.format(seed), compress=True)\n",
    "joblib.dump(demographics_scaler, \"scalers/train/demographics_std_scaler_{}.bin\".format(seed), compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that scaling did not impose any NAs\n",
    "assert train_df['tempc_imp'].isna().sum() == train_df['tempc_imp_scaled'].isna().sum()\n",
    "assert train_df['admission_age'].isna().sum() == train_df['admission_age_scaled'].isna().sum()\n",
    "\n",
    "assert test_df['tempc_imp'].isna().sum() == test_df['tempc_imp_scaled'].isna().sum()\n",
    "assert test_df['admission_age'].isna().sum() == test_df['admission_age_scaled'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_impute = scaled_varnames + demo_scaled_varnames\n",
    "knn_imputed_vars = list(map(lambda x: x + '_impknn', vars_to_impute))\n",
    "\n",
    "imputer = KNNImputer() # use default of n=5, weights='uniform', metric='nan_euclidean'\n",
    "imputed = imputer.fit_transform(train_df[vars_to_impute].to_numpy())\n",
    "imputed_test = imputer.transform(test_df[vars_to_impute].to_numpy())\n",
    "imputed_val = imputer.transform(val_df[vars_to_impute].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, knn_imputed_vars] = imputed\n",
    "val_df.loc[:,knn_imputed_vars] = imputed_val\n",
    "test_df.loc[:, knn_imputed_vars] = imputed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_imputed_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that scaling did not impose any NAs\n",
    "assert train_df['tempc_imp'].isna().sum() > train_df['tempc_imp_scaled_impknn'].isna().sum()\n",
    "assert train_df['admission_age'].isna().sum() > train_df['admission_age_scaled_impknn'].isna().sum()\n",
    "\n",
    "assert test_df['tempc_imp'].isna().sum() > test_df['tempc_imp_scaled_impknn'].isna().sum()\n",
    "assert test_df['admission_age'].isna().sum() > test_df['admission_age_scaled_impknn'].isna().sum()\n",
    "\n",
    "assert val_df['tempc_imp'].isna().sum() > val_df['tempc_imp_scaled_impknn'].isna().sum()\n",
    "assert val_df['admission_age'].isna().sum() > val_df['admission_age_scaled_impknn'].isna().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(imputer, \"scalers/train/knn_imputer_{}.bin\".format(seed), compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adult_ibw(row):\n",
    "    if row.gender == 'M':\n",
    "        return 50 + 0.91 * (row.height - 152.4)\n",
    "    elif row.gender == 'F':\n",
    "        return 45 + 0.91 * (row.height - 152.4)\n",
    "    else:\n",
    "        return np.NaN\n",
    "\n",
    "\n",
    "for df_name, df in (('train', train_df), ('test', test_df), ('validation', val_df)):\n",
    "    df['adult_ibw_imp_knn'] = df.apply(adult_ibw, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in (('train', train_df), ('test', test_df), ('validation', val_df)):\n",
    "    df = df[~df['adult_ibw_imp_knn'].isna()]\n",
    "    if df_name == 'train':\n",
    "        df.to_csv('../mimic-data/{}_imputed_{}.csv'.format(df_name, seed))\n",
    "    else:\n",
    "        df.to_csv('../mimic-data/{}_ffilled_{}.csv'.format(df_name, seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_l = pd.read_csv('../mimic-data/train_imputed_{}.csv'.format(seed))\n",
    "test_df_l = pd.read_csv('../mimic-data/test_ffilled_{}.csv'.format(seed))\n",
    "val_df_l = pd.read_csv('../mimic-data/validation_ffilled_{}.csv'.format(seed))\n",
    "\n",
    "train_df_l.shape[0], test_df_l.shape[0], val_df_l.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = pd.concat([train_df_l, test_df_l, val_df_l])\n",
    "selection['los_icu'] = pd.to_timedelta(selection['los_icu'])\n",
    "selection['hadm_duration'] = pd.to_timedelta(selection['hadm_duration'])\n",
    "selected_patients = selection.drop_duplicates(subset='icustay_id', keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Population demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:<30} {}\".format(\"Included patients\", selection.nunique()))\n",
    "print(\"{:<30} {}\".format(\"Included stays\", selection.icustay_id.nunique()))\n",
    "print(\"{:<30} {:.2f} +- ({:.2f}-{:.2f})\".format(\n",
    "    \"Age, years median (q1-q3)\",\n",
    "    selected_patients.admission_age.median(),\n",
    "    selected_patients.admission_age.quantile(.25),\n",
    "    selected_patients.admission_age.quantile(.75)\n",
    "))\n",
    "print(\"{:<30} {:.2f} ±{:.2f}\".format(\n",
    "    \"Adm. Weight mean ±std\",\n",
    "    selected_patients.weight.mean(),\n",
    "    selected_patients.weight.std()\n",
    "))\n",
    "print(\"{:<30} {:.2f} ±{:.2f}\".format(\n",
    "    \"Pred. IBW mean ±std\",\n",
    "    selected_patients.adult_ibw.mean(),\n",
    "    selected_patients.adult_ibw.std()\n",
    "))\n",
    "gender_counts = selected_patients.gender.value_counts\n",
    "print(\"{:<30} {} ({:.2f}%)\".format(\"Female\", (selected_patients.gender == 'F').sum(), 100*(selected_patients.gender == 'F').sum() / selected_patients.shape[0]))\n",
    "print(\"{:<30} {} ({:.2f}%)\".format(\"Male\", (selected_patients.gender == 'M').sum(), 100*(selected_patients.gender == 'M').sum() / selected_patients.shape[0]))\n",
    "print(\"{:<30} {:.2f}%\".format(\"90-day mortality\", (selected_patients.mort90day == 't').sum() / selected_patients.shape[0] / 0.01))\n",
    "print(\"{:<30} {:.2f}%\".format(\"hosp mortality\", (selected_patients.hospmort == 't').sum() / selected_patients.shape[0] / 0.01))\n",
    "los_icu_median_days = selected_patients.los_icu.astype(\"timedelta64[ms]\").median().components.days + selected_patients.los_icu.median().components.hours / 24\n",
    "los_icu_q1_days =  selected_patients.los_icu.astype(\"timedelta64[ms]\").quantile(.25).components.days + selected_patients.los_icu.quantile(.25).components.hours / 24\n",
    "los_icu_q3_days =  selected_patients.los_icu.astype(\"timedelta64[ms]\").quantile(.75).components.days + selected_patients.los_icu.quantile(.75).components.hours / 24\n",
    "print(\"{:<30} {:.2f} ({:.2f}-{:.2f})\".format(\n",
    "    \"LOS ICU median (q1-q3)\",\n",
    "    los_icu_median_days,\n",
    "    los_icu_q1_days,\n",
    "    los_icu_q3_days\n",
    "))\n",
    "los_hosp_median_days = selected_patients.hadm_duration.astype(\"timedelta64[ms]\").median().components.days + selected_patients.hadm_duration.astype(\"timedelta64[ms]\").median().components.hours / 24\n",
    "los_hosp_q1_days =  selected_patients.hadm_duration.quantile(.25).components.days + selected_patients.hadm_duration.astype(\"timedelta64[ms]\").quantile(.25).components.hours / 24\n",
    "los_hosp_q3_days =  selected_patients.hadm_duration.quantile(.75).components.days + selected_patients.hadm_duration.astype(\"timedelta64[ms]\").quantile(.75).components.hours / 24\n",
    "print(\"{:<30} {:.2f} ({:.2f}-{:.2f})\".format(\"LOS hosp median (q1-q3)\", los_hosp_median_days, los_hosp_q1_days, los_icu_q3_days))\n",
    "print(\"{:<30} {:.1f} ±{:.2f}\".format(\"PEEP cmH2O mean ±std\", selection.peep.mean(), selection.peep.std()))\n",
    "print(\"{:<30} {:.2f} ±{:.2f}\".format(\"FiO2, % mean ±std\", selection.fio2.mean(), selection.fio2.std()))\n",
    "selection['vt_mlkg'] = selection.tidal_volume / selection.adult_ibw_imp_knn\n",
    "print(\"{:<30} {:.2f} ±{:.2f}\".format(\"Vt, mL/kg (PIBW) mean ±std\", selection['vt_mlkg'].mean(), selection['vt_mlkg'].std()))\n",
    "print(\"{:<30} {:.2f} ±{:.2f}\".format(\n",
    "    \"SOFA points @ adm. mean ±std\",\n",
    "    selected_patients.sofa.mean(),\n",
    "    selected_patients.sofa.std()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 35,556 decision time instances according to peine et al, Table 2a Supplementary material.\n",
    "selection.shape[0], selection.shape[0] / selected_patients.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper reports inclusion of 35,556 decision time instances for MIMIC-III, this gives an average trajectory length of\n",
    "35556 / 11443"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_patients.vaso_total.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
