{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "from collections.abc import Iterable\n",
    "import functools\n",
    "import itertools\n",
    "import operator\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from pprint import pprint\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import poisson\n",
    "from scipy.sparse import hstack, vstack, csr_matrix\n",
    "import scipy\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from config import demographics, vital_sign_vars, lab_vars, treatment_vars, vent_vars, guideline_vars, ffill_windows_clinical, SAMPLE_TIME_H\n",
    "from config import fio2_bins, peep_bins, tv_bins\n",
    "import safety\n",
    "import utils\n",
    "from utils import to_discrete_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers_dir = '../preprocessing/scalers/'\n",
    "data_dir = '../mimic-data/'\n",
    "seed = 12\n",
    "\n",
    "SHAPING = False\n",
    "COMPLIANCE_SCALAR = 0.0 # should be in [0, 5]\n",
    "UNSAFETY_PROB = 1.0 # should be in [0.0,1.0] or {0,0, 1.0} until safety probs implemented\n",
    "GAMMA = 0.99\n",
    "n_states = 650 # according to Peine's paper\n",
    "CREATE_TRAJS = True\n",
    "if not SHAPING:\n",
    "    shaping_name = 'unshaped'\n",
    "else:\n",
    "    shaping_name = SHAPING + '-' + str(COMPLIANCE_SCALAR )\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "all_var_types = [\n",
    "    vital_sign_vars,\n",
    "    lab_vars,\n",
    "    treatment_vars,\n",
    "    vent_vars,\n",
    "    guideline_vars,\n",
    "]\n",
    "all_vars = functools.reduce(operator.add, all_var_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    train_set = pd.read_csv(data_dir + 'train_imputed_{}.csv'.format(seed))\n",
    "    clinical_train_scaler = joblib.load(scalers_dir + 'train/clinical_std_scaler_{}.bin'.format(seed))\n",
    "    demographics_train_scaler = joblib.load(scalers_dir + 'train/demographics_std_scaler_{}.bin'.format(seed))\n",
    "\n",
    "    test_set = pd.read_csv(data_dir + 'test_ffilled_{}.csv'.format(seed))\n",
    "    val_set = pd.read_csv(data_dir + 'validation_ffilled_{}.csv'.format(seed))\n",
    "\n",
    "    knn_imputer = joblib.load(scalers_dir + 'train/knn_imputer_{}.bin'.format(seed))\n",
    "    scalers = {}\n",
    "    for var in all_vars + demographics:\n",
    "        try:\n",
    "            scalers[var] = joblib.load(scalers_dir + 'train/{}_std_scaler_{}.bin'.format(var, seed))\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    # TODO: FIX for test and validation sets: scaling (w/ train scaler?), knn imputation (w/ train clusterer), scaling (w/ train scaler?) and clustering (w/ train clusterer)\n",
    "    # for df in (test_set, val_set):\n",
    "    #     vars_to_impute = []\n",
    "    #     for var in all_vars:\n",
    "    #         if pd.api.types.is_numeric_dtype(df[var]):\n",
    "    #             ffilled = var + '_imp'\n",
    "    #             non_nas = df[~df[ffilled].isna()][ffilled].to_numpy().reshape(-1,1)\n",
    "    #             scaled = scalers[var].transform(non_nas)\n",
    "    #             scaledvarname = ffilled + '_scaled'\n",
    "    #             df[scaledvarname] = pd.Series(scaled.reshape(1,-1)[0])\n",
    "    #             vars_to_impute.append(scaledvarname)\n",
    "    #     for var in demographics:\n",
    "    #         if pd.api.types.is_numeric_dtype(df[var]):\n",
    "    #             non_nas = df[~df[var].isna()][var].to_numpy().reshape(-1, 1)\n",
    "    #             scaled = scalers[var].transform(df[var].to_numpy().reshape(-1,1))\n",
    "    #             scaledvarname = var + '_scaled'\n",
    "    #             df[scaledvarname] = pd.Series(scaled.reshape(1,-1)[0])\n",
    "    #             vars_to_impute.append(scaledvarname)\n",
    "    #     knn_imputed_vars = list(map(lambda x: x + '_impknn', vars_to_impute))\n",
    "    #     imputed = knn_imputer.transform(df[vars_to_impute].to_numpy())\n",
    "    #     df.loc[:, knn_imputed_vars] = imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    # NOTE: these have not been rescaled after imputation\n",
    "    test_set.to_csv(data_dir + 'test_imputed_{}.csv'.format(seed))\n",
    "    val_set.to_csv(data_dir + 'validation_imputed_{}.csv'.format(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    var_to_state_var = {}\n",
    "    for var in all_vars + demographics:\n",
    "        if var not in ('vent_duration_h', 'hospmort', 'mort90day'):\n",
    "            if var + '_imp_scaled_impknn' in train_set.columns:\n",
    "                var_to_state_var[var] = var + '_imp_scaled_impknn'\n",
    "            elif var + '_scaled_impknn' in train_set.columns:\n",
    "                var_to_state_var[var] = var + '_scaled_impknn'\n",
    "\n",
    "    # TODO: validate if this contains all vars\n",
    "    state_variables = list(var_to_state_var.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    longest_sv = max(map(len, state_variables))\n",
    "    for sv in state_variables:\n",
    "        print((\"{:<\" + str(longest_sv) +\"}: {:.4f} {:.4f} {:.4f}\").format(sv, train_set[sv].mean(), train_set[sv].median(), train_set[sv].var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    # NOTE: mirror the changes made here in the 'unscale' logic\n",
    "    rescaler = StandardScaler()\n",
    "\n",
    "    rescaled = rescaler.fit_transform(train_set[state_variables])\n",
    "    rs_state_varnames = [var + '_rescaled' for var in state_variables]\n",
    "    train_set[rs_state_varnames] = rescaled\n",
    "\n",
    "    rescaled_test = rescaler.transform(test_set[state_variables])\n",
    "    test_set[rs_state_varnames] = rescaled_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    clusterer = KMeans(n_clusters=n_states)\n",
    "    train_clusters = clusterer.fit_predict(train_set[rs_state_varnames])\n",
    "    test_clusters = clusterer.predict(test_set[rs_state_varnames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    sns.histplot(train_clusters, bins=n_states)\n",
    "    plt.title('Train set state cluster distr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    sns.histplot(test_clusters, bins=n_states)\n",
    "    plt.title('Test set state cluster distr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    clusters = pd.Series(train_clusters)\n",
    "    test_clusters_series = pd.Series(test_clusters)\n",
    "    clusters.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    longest_sv = max(map(len, rs_state_varnames))\n",
    "    for sv in rs_state_varnames:\n",
    "        print((\"{:<\" + str(longest_sv) +\"}: {:.4f} {:.4f} {:.4f}\").format(sv, train_set[sv].mean(), train_set[sv].median(), train_set[sv].var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(train_set[rs_state_varnames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    #pca = PCA(n_components=2)\n",
    "    #pca_result = pca.fit_transform(test_set[rs_state_varnames])\n",
    "    pca_result = pca.transform(test_set[rs_state_varnames])\n",
    "    sns.scatterplot(x=pca_result[:,0], y=pca_result[:,1], c=test_clusters)\n",
    "    plt.title('Clustering Result Test (k={}, rescaled, PCA)'.format(n_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    print(\"\"\"silhouette: {}, [-1, 1], higher is better\n",
    "    calinksi-harabasz: {}, higher is better\n",
    "    davies-bouldin: {}, [0, âˆž], lower is better\"\"\".format(\n",
    "        metrics.silhouette_score(train_set[rs_state_varnames], train_clusters),\n",
    "        metrics.calinski_harabasz_score(train_set[rs_state_varnames], train_clusters),\n",
    "        metrics.davies_bouldin_score(train_set[rs_state_varnames], train_clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    train_set['state'] = clusters\n",
    "    test_set['state'] = test_clusters_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unscale - back to original and interpretable space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    # This step is spurious for now, as there is no imputation after rescaling\n",
    "    un_rescaled_state_vars = rescaler.inverse_transform(train_set[rs_state_varnames])\n",
    "    un_rescaled_state_vars_test = rescaler.inverse_transform(test_set[rs_state_varnames])\n",
    "\n",
    "    clinical_scaled_varnames = [var + '_impknn' for var in joblib.load('../preprocessing/scalers/train/clinical_std_varnames_{}.bin'.format(seed))]\n",
    "    demographics_scaled_varnames = [var + '_impknn' for var in joblib.load('../preprocessing/scalers/train/demographics_std_varnames_{}.bin'.format(seed))]\n",
    "    unscaled_clinical_vars = clinical_train_scaler.inverse_transform(train_set[clinical_scaled_varnames])\n",
    "    unscaled_clinical_vars_test = clinical_train_scaler.inverse_transform(test_set[clinical_scaled_varnames])\n",
    "    unscaled_demo_vars = demographics_train_scaler.inverse_transform(train_set[demographics_scaled_varnames])\n",
    "    unscaled_demo_vars_test = demographics_train_scaler.inverse_transform(test_set[demographics_scaled_varnames])\n",
    "\n",
    "\n",
    "    unscaled_clinical_varnames = [var + '_unscaled' for var in clinical_scaled_varnames]\n",
    "    unscaled_demographics_varnames = [var + '_unscaled' for var in demographics_scaled_varnames]\n",
    "    train_set[unscaled_clinical_varnames] = unscaled_clinical_vars\n",
    "    test_set[unscaled_clinical_varnames] = unscaled_clinical_vars_test\n",
    "    train_set[unscaled_demographics_varnames] = unscaled_demo_vars\n",
    "    test_set[unscaled_demographics_varnames] = unscaled_demo_vars_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    # validation unscaling\n",
    "    #  * does not introduce NaNs\n",
    "    #  * does not change known values (by some error margin)\n",
    "    clinical_varname_pairs = list(zip(unscaled_clinical_varnames, map(lambda x: x.replace(\"_imp_scaled_impknn\", \"\"), clinical_scaled_varnames)))\n",
    "    demo_varname_pairs = list(zip(unscaled_demographics_varnames, map(lambda x: x.replace(\"_scaled_impknn\", \"\"), demographics_scaled_varnames)))\n",
    "    all_varname_pairs = clinical_varname_pairs + demo_varname_pairs\n",
    "    for (unscaled, original) in all_varname_pairs:\n",
    "        print(unscaled, original)\n",
    "        # check that the number of NaNs for unscaled is less or equal than original\n",
    "        assert train_set[unscaled].isna().sum() <= train_set[original].isna().sum(), \"Unscaling has introduced NaNs in train set\"\n",
    "        assert test_set[unscaled].isna().sum() <= test_set[original].isna().sum(), \"Unscaling has introduced NaNs in test set\"\n",
    "        # calculate difference between unscaled and original value\n",
    "        difference = np.absolute(train_set[~train_set[unscaled].isna()][unscaled] - train_set[~train_set[unscaled].isna()][original])\n",
    "        difference_test = np.absolute(test_set[~test_set[unscaled].isna()][unscaled] - test_set[~test_set[unscaled].isna()][original])\n",
    "        # difference should be small OR the original should be NaN.\n",
    "        assert ((difference < 0.0001) | train_set[~train_set[unscaled].isna()][original].isna()).all(), \"Unscaling introduced errors for variable '{}' in train set\".format(original)\n",
    "        assert ((difference_test < 0.0001) | test_set[~test_set[unscaled].isna()][original].isna()).all(), \"Unscaling introduced errors for variable '{}' in test set\".format(original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute derived values\n",
    "- normalized tidal volume: tv in ml/Kg of adult ideal body weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    # derived variables based on imputed / normalized values\n",
    "    # - tidal_volume\n",
    "    train_set['tv_derived'] = train_set['tidal_volume_imp_scaled_impknn_unscaled'] / train_set['adult_ibw_scaled_impknn_unscaled']\n",
    "    test_set['tv_derived'] = test_set['tidal_volume_imp_scaled_impknn_unscaled'] / test_set['adult_ibw_scaled_impknn_unscaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if CREATE_TRAJS:\n",
    "#     (train_adult_ibwdult_ibw_scaled_impknn_unscaled < 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action space discretization\n",
    "The action space consists of three dimensions:\n",
    "* Tidal volume (ml/Kg), where Kg refers to the ideal body weight\n",
    "* PEEP (cmH20) positive end-expiration pressure\n",
    "* FiO2 (%) fraction of inspired oxygen\n",
    "\n",
    "These three dimensions are each discretized into 7 bins, rendering a total of 7^3 = 343 actions.  \n",
    "The action bins are defined per dimension and according to Peine in ``config.py``.\n",
    "Here we map all possible actions to a discrete identifier action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    # a definition of all actions\n",
    "    action_bin_definition = list(itertools.product(tv_bins, fio2_bins, peep_bins))\n",
    "    # the lower bounds for the three variables in the action space\n",
    "    lower_bounds = [[var[0] for var in ranges] for ranges in action_bin_definition] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    assert to_discrete_action(1, 25, 0) == 0, \"the first action does not have id 0\"             # the first action\n",
    "    assert to_discrete_action(1, 25, 5) == 1, \"the second action does not have id 1\"             # the second action\n",
    "    assert to_discrete_action(1, 25, sys.maxsize) == 6, \"the last action in the third dimension does not have id 6\"   # very high value for 3rd dimension (peep)\n",
    "    assert to_discrete_action(2.5, 25, 0) == 7*7, \"the last action in the second dimension does not have id 7*7\"         # second bin for the first dimension (tv)\n",
    "    assert to_discrete_action(sys.maxsize,\n",
    "                              sys.maxsize,\n",
    "                              sys.maxsize) == 7**3 - 1, \"an action with very high values does not have id 7**3 -1\"   # very high value for all bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    tv_clinical = 'tv_derived'\n",
    "    peep_clinical = 'peep_imp_scaled_impknn_unscaled'\n",
    "    fio2_clinical = 'fio2_imp_scaled_impknn_unscaled'\n",
    "\n",
    "    discretized_actions = train_set.apply(lambda x: to_discrete_action(x[tv_clinical], x[fio2_clinical], x[peep_clinical]), axis=1)\n",
    "    discretized_actions_test = test_set.apply(lambda x: to_discrete_action(x[tv_clinical], x[fio2_clinical], x[peep_clinical]), axis=1)\n",
    "\n",
    "    train_set['action_discrete'] = discretized_actions\n",
    "    test_set['action_discrete'] = discretized_actions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    # sanity spot check -- do the sample values fall in the ranges?\n",
    "    train_set[[tv_clinical, peep_clinical, fio2_clinical]].iloc[0], discretized_actions[0], action_bin_definition[discretized_actions[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of action and state-action distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    observed_action_counts = discretized_actions.value_counts()\n",
    "    action_info = {}\n",
    "    action_counts = []\n",
    "    for action_id in range(len(action_bin_definition)):\n",
    "        count = observed_action_counts[action_id] if action_id in observed_action_counts else 0\n",
    "        action_info[action_id] = (action_bin_definition, count)\n",
    "        action_counts.append(count)\n",
    "    action_counts = pd.Series(action_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    action_counts.hist(bins=action_counts.nunique(), log=True)\n",
    "    plt.title('action selection distribution train (log scale)')\n",
    "    print(\"# possible actions: {}\".format(len(action_bin_definition)))\n",
    "    print(\"actions never chosen: {}\".format((action_counts == 0).sum()))\n",
    "    most_chosen_action = action_bin_definition[discretized_actions.mode()[0]]\n",
    "    print(\"most chosen action: (tv: {}, peep:{}, fio2: {}), chosen {} times\".format(*most_chosen_action, discretized_actions.value_counts().max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    observed_action_counts_test = discretized_actions_test.value_counts()\n",
    "    action_info_test = {}\n",
    "    action_counts_test = []\n",
    "    for action_id in range(len(action_bin_definition)):\n",
    "        count = observed_action_counts_test[action_id] if action_id in observed_action_counts_test else 0\n",
    "        action_info_test[action_id] = (action_bin_definition, count)\n",
    "        action_counts_test.append(count)\n",
    "    action_counts_test = pd.Series(action_counts_test)\n",
    "    action_counts.hist(bins=action_counts_test.nunique(), log=True)\n",
    "    plt.title('action selection distribution test (log scale)')\n",
    "    print(\"# possible actions: {}\".format(len(action_bin_definition)))\n",
    "    print(\"actions never chosen: {}\".format((action_counts_test == 0).sum()))\n",
    "    most_chosen_action = action_bin_definition[discretized_actions_test.mode()[0]]\n",
    "    print(\"most chosen action: (tv: {}, peep:{}, fio2: {}), chosen {} times\".format(*most_chosen_action, discretized_actions_test.value_counts().max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    # state-action pair\n",
    "    state_actions = list(itertools.product(range(n_states), range(len(action_bin_definition)),))\n",
    "    print(\"{} states x {} actions = {} state-action pairs\".format(n_states, len(action_bin_definition), len(state_actions)))\n",
    "    state_action_ids = list(map(lambda x: \"{}-{}\".format(x[0], x[1]), state_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    train_set['terminal'] = False\n",
    "    train_set.loc[train_set.groupby('icustay_id').tail(1).index, 'terminal'] = True\n",
    "\n",
    "    train_set['state_action_id'] = train_set.agg('{0[state]}-{0[action_discrete]}'.format, axis=1)\n",
    "    train_set['next_state'] = train_set.state.astype(str).shift(-1)\n",
    "    # create special terminal states for both outcomes\n",
    "    terminal_mort = train_set['state'].max() + 1\n",
    "    terminal_nonmort = train_set['state'].max() + 2\n",
    "\n",
    "    train_set.loc[train_set.terminal & (train_set.mort90day == 't'), 'next_state'] = terminal_mort\n",
    "    train_set.loc[train_set.terminal & (train_set.mort90day == 'f'), 'next_state'] = terminal_nonmort\n",
    "\n",
    "    # State-Action-NextState_ID\n",
    "    train_set['sans_id'] = train_set.agg('{0[state_action_id]}-{0[next_state]}'.format, axis=1)\n",
    "    observed_saction_counts = train_set['state_action_id'].value_counts()\n",
    "    saction_info = {}\n",
    "    saction_counts = []\n",
    "    for i, saction_id in enumerate(state_action_ids):\n",
    "        count = observed_saction_counts[saction_id] if saction_id in observed_saction_counts else 0\n",
    "        state, action = saction_id.split('-')\n",
    "        saction_info[saction_id] = (saction_id, i, count, int(state), int(action), action_bin_definition[int(action)], )\n",
    "        saction_counts.append(count)\n",
    "    saction_counts = pd.Series(saction_counts)\n",
    "    # saction_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    test_set['state_action_id'] = test_set.agg('{0[state]}-{0[action_discrete]}'.format, axis=1)\n",
    "    test_set['next_state'] = test_set.state.astype(str).shift(-1)\n",
    "    # State-Action-NextState_ID\n",
    "    test_set['sans_id'] = test_set.agg('{0[state_action_id]}-{0[next_state]}'.format, axis=1)\n",
    "    observed_saction_counts_test = test_set['state_action_id'].value_counts()\n",
    "    \n",
    "    \n",
    "    test_set.loc[train_set.terminal & (train_set.mort90day == 't'), 'next_state'] = terminal_mort\n",
    "    test_set.loc[train_set.terminal & (train_set.mort90day == 'f'), 'next_state'] = terminal_nonmort\n",
    "    \n",
    "    saction_info_test = {}\n",
    "    saction_counts_test = []\n",
    "    for i, saction_id in enumerate(state_action_ids):\n",
    "        count = observed_saction_counts_test[saction_id] if saction_id in observed_saction_counts_test else 0\n",
    "        state, action = saction_id.split('-')\n",
    "        saction_info_test[saction_id] = (saction_id, i, count, int(state), int(action), action_bin_definition[int(action)], )\n",
    "        saction_counts_test.append(count)\n",
    "    saction_counts_test = pd.Series(saction_counts_test)\n",
    "    # saction_info_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    saction_info_df = pd.DataFrame([[*v,] for k, v in  saction_info.items()], columns=['saction_id', 'index', 'count', 'state', 'action', 'action'])\n",
    "    saction_info_df_test = pd.DataFrame([[*v,] for k, v in  saction_info_test.items()], columns=['saction_id', 'index', 'count', 'state', 'action', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    print(\"# state-action pairs: {}\".format(len(state_action_ids)))\n",
    "    print(\"# state-action pairs never visited: {}\".format((saction_counts == 0).sum()))\n",
    "    most_frequent_saction = saction_info_df[saction_info_df.index == saction_counts.argmax()].to_numpy()[0]\n",
    "    most_chosen_action = action_bin_definition[discretized_actions.mode()[0]]\n",
    "    saction_counts.hist(bins=saction_counts.nunique(), log=True)\n",
    "    plt.title(\"Histogram of state-action pair occurences in train set\")\n",
    "    print(\"most visited state-action pair in train: (tv: {}, peep:{}, fio2: {}, state: {}), visited {} times\".format(*most_chosen_action, most_frequent_saction[3], most_frequent_saction[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_TRAJS:\n",
    "    most_frequent_saction_test = saction_info_df[saction_info_df.index == saction_counts.argmax()].to_numpy()[0]\n",
    "    most_chosen_action_test = action_bin_definition[discretized_actions.mode()[0]]\n",
    "    saction_counts_test.hist(bins=saction_counts_test.nunique(), log=True)\n",
    "    plt.title(\"Histogram of state-action pair occurences in test set\")\n",
    "    print(\"most visited state-action pair in train: (tv: {}, peep:{}, fio2: {}, state: {}), visited {} times\".format(*most_chosen_action_test, most_frequent_saction_test[3], most_frequent_saction_test[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CREATE_TRAJS:\n",
    "    test_set = pd.read_csv('data/test_{}_traj_{}.csv'.format(shaping_name, seed))\n",
    "    train_set = pd.read_csv('data/train_{}_traj_{}.csv'.format(shaping_name, seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['traj_count'] = train_set.sort_values(['icustay_id', 'start_time']).groupby('icustay_id').cumcount()\n",
    "train_set['traj_count_inv'] = train_set.sort_values(['icustay_id', 'start_time']).groupby('icustay_id').cumcount(ascending=False)\n",
    "train_set['traj_len'] = train_set.groupby('icustay_id')['traj_count'].transform('max')\n",
    "train_set['traj_len'] += 1\n",
    "\n",
    "trajectory_lengths = train_set.groupby('icustay_id').traj_count.max() + 1\n",
    "trajectory_lengths.hist()\n",
    "plt.title('Trajectory lengths train')\n",
    "plt.xlabel('# of 4h time steps')\n",
    "plt.ylabel('# trajectories')\n",
    "trajectory_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['traj_count'] = test_set.sort_values(['icustay_id', 'start_time']).groupby('icustay_id').cumcount()\n",
    "test_set['traj_count_inv'] = test_set.sort_values(['icustay_id', 'start_time']).groupby('icustay_id').cumcount(ascending=False)\n",
    "test_set['traj_len'] = test_set.groupby('icustay_id')['traj_count'].transform('max')\n",
    "test_set['traj_len'] += 1\n",
    "\n",
    "trajectory_lengths_test = test_set.groupby('icustay_id').traj_count.max() + 1\n",
    "trajectory_lengths_test.hist()\n",
    "plt.title('Trajectory lengths test')\n",
    "plt.xlabel('# of 4h time steps')\n",
    "plt.ylabel('# trajectories')\n",
    "trajectory_lengths_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminals = train_set.groupby('icustay_id').max('traj_count').reset_index().set_index('Unnamed: 0')['traj_count']\n",
    "terminals = (~terminals.isna()).rename('terminal')\n",
    "if 'terminal' in train_set.columns:\n",
    "    # need to remove this column if it already exists\n",
    "    del(train_set['terminal'])\n",
    "train_set = train_set.join(terminals, on='Unnamed: 0', how='left')\n",
    "train_set['terminal'] = train_set.terminal.fillna(False)\n",
    "train_set['nterminal'] = train_set.terminal.shift(-1).fillna(False)\n",
    "\n",
    "terminals = test_set.groupby('icustay_id').max('traj_count').reset_index().set_index('Unnamed: 0')['traj_count']\n",
    "terminals = (~terminals.isna()).rename('terminal')\n",
    "if 'terminal' in test_set.columns:\n",
    "    # need to remove this column if it already exists\n",
    "    del(test_set['terminal'])\n",
    "test_set = test_set.join(terminals, on='Unnamed: 0', how='left')\n",
    "test_set['terminal'] = test_set.terminal.fillna(False)\n",
    "test_set['nterminal'] = test_set.terminal.shift(-1).fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_set.groupby('icustay_id').terminal.sum().max() == 1, \"Every icustay should have exactly 1 terminal state\"\n",
    "assert test_set.groupby('icustay_id').terminal.sum().max() == 1, \"Every icustay should have exactly 1 terminal state\"\n",
    "assert train_set.groupby('icustay_id').terminal.sum().min() == 1, \"Every icustay should have exactly 1 terminal state\"\n",
    "assert test_set.groupby('icustay_id').terminal.sum().min() == 1, \"Every icustay should have exactly 1 terminal state\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate s,s',a tuples for transition matrix approximation\n",
    "  \n",
    "# calculate joint state-action probabilities\n",
    "pr_state_action = pd.DataFrame((train_set.state_action_id.value_counts() / train_set.shape[0])).reset_index().rename(columns={'state_action_id': 'sa_id', 'count': 'pr_sa'})\n",
    "\n",
    "\n",
    "# calculate joint s-s'-a probabilities\n",
    "sans_id = '' # state-action-nstate id\n",
    "pr_nstate_state_action = pd.DataFrame(train_set.sans_id.value_counts() / train_set.shape[0]).reset_index().rename(columns={'index': 'sans_id', 'count': 'pr_sans'})\n",
    "pr_nstate_state_action['sa_id'] = list(map(lambda x: '-'.join(x[:2]), pr_nstate_state_action.sans_id.str.split('-')))\n",
    "pr_sans = pr_nstate_state_action.merge(pr_state_action, on='sa_id', how='inner').set_index('sans_id')\n",
    "\n",
    "# calculate transition probabilities\n",
    "pr_trans = pr_sans.pr_sans / pr_sans.pr_sa\n",
    "pr_trans = pd.DataFrame(pr_trans).rename(columns={0: 'pr_t'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_trans.pr_t.hist()\n",
    "pr_trans\n",
    "plt.title('Distribution of nonzero transition probabilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_sans = pd.DataFrame(pr_trans.index.str.split('-').tolist()).rename(columns={0:'state', 1: 'action', 2: 'nstate'})\n",
    "split_sans.state = split_sans.state.astype(int)\n",
    "split_sans.action = split_sans.action.astype(int)\n",
    "split_sans.nstate = split_sans.nstate.astype(int)\n",
    "assert split_sans.action.max() <= (7**3 - 1), \"the highest action id exceeds the number of actions\" # number of discrete actions\n",
    "pr_trans = pr_trans.reset_index().merge(split_sans, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all transition probabilities for a state-action pair should sum to 1, there could be some rounding errors\n",
    "print(\"{:.5f}% of state-action's probs does not sum to 1, max diff to 1 is {}\".format(\n",
    "    (~(pr_trans.groupby(['state', 'action']).pr_t.sum() >= 1.0)).to_numpy().mean(),\n",
    "    np.diff(pr_trans.groupby(['state', 'action']).pr_t.sum(), 1).max(),\n",
    "))\n",
    "assert np.diff(pr_trans.groupby(['state', 'action']).pr_t.sum(), 1).max() < 1e-15, \"transition probabilities for a state-action pair should sum (approx.) to 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diff(pr_trans.groupby(['state', 'action']).pr_t.sum(), 1).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_trans.pr_t.hist()\n",
    "plt.title('Histogram of nonzero transition probabilities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immediate reward definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compliance_to_potential(compliance):\n",
    "    return compliance * COMPLIANCE_SCALAR\n",
    "\n",
    "def potential_diff(x):\n",
    "    if np.isnan(x.iloc[1]):\n",
    "        p1 = 0.0 # see Grzes, AAMAS 2017\n",
    "    else:\n",
    "        p1 = GAMMA * x.iloc[1]\n",
    "    return p1 - x.iloc[0]\n",
    "\n",
    "train_set['reward'] = 0\n",
    "test_set['reward'] = 0\n",
    "train_set.loc[train_set.terminal & (train_set.mort90day == 't'), 'reward'] = -100\n",
    "test_set.loc[test_set.terminal & (test_set.mort90day == 't'), 'reward'] = -100\n",
    "train_set.loc[train_set.terminal & (train_set.mort90day == 'f'), 'reward'] = 100\n",
    "test_set.loc[test_set.terminal & (test_set.mort90day == 'f'), 'reward'] = 100\n",
    "\n",
    "\n",
    "assert train_set.reward.isna().sum() == 0, \"All immediate rewards should be set\"\n",
    "assert test_set.reward.isna().sum() == 0, \"All immediate rewards should be set\"\n",
    "assert train_set[train_set.terminal].reward.isin({-100, 100}).all(), \"Transitions to terminal states should have value -100 or 100\"\n",
    "assert test_set[test_set.terminal].reward.isin({-100, 100}).all(), \"Transitions to terminal states should have value -100 or 100\"\n",
    "\n",
    "if SHAPING:\n",
    "    # TODO FdH: implement shaping\n",
    "    if SHAPING == 'avgpotential2':\n",
    "        train_set['compliance'] = safety.state_compliance_clinical(train_set, safety.avg_clinical_timestep)\n",
    "    elif SHAPING == 'allpotential':\n",
    "        train_set['compliance'] = safety.state_compliance_clinical(train_set, safety.all_clinical_timestep)\n",
    "    else:\n",
    "        raise ValueError('Unknown shaping approach')\n",
    "    train_set['potential'] = compliance_to_potential(train_set['compliance'])\n",
    "    if 'potential' in SHAPING:\n",
    "        train_set['shaping_reward_unshift'] = train_set.groupby('icustay_id').rolling(window=2)['potential'].apply(potential_diff).fillna(0.0).reset_index().set_index('level_1')['potential']\n",
    "        train_set['shaping_reward'] = train_set['shaping_reward_unshift'].shift(-1)\n",
    "        train_set.loc[train_set.terminal, 'shaping_reward'] = train_set['potential']\n",
    "    elif 'base' in SHAPING:\n",
    "        train_set['shaping_reward'] = train_set['potential']\n",
    "    \n",
    "    train_set['reward'] = train_set.reward + train_set.shaping_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. compute average immediate reward for each s-s'-a with support from the data.\n",
    "# 2. where -100 if terminal and 90 day mortality, +100 if terminal and 90 day mortality, 0 otherwise\n",
    "# 3. element-wise multiply with transition matrix pr_trans\n",
    "# 4. sum over s' dimension to obtain R(s, a) < sum because transition matrix has probabilities\n",
    "\n",
    "# step 1: compute average immediate reward for each s-s'-a with support from the data.\n",
    "r_sans = train_set.groupby('sans_id')['reward'].mean()\n",
    "r_sans.hist()\n",
    "plt.title('Distribution of rewards in s-a-s-r matrix')\n",
    "split_r_sans = pd.DataFrame(r_sans.index.str.split('-').tolist()).rename(columns={0:'state', 1: 'action', 2: 'nstate'})\n",
    "split_r_sans.state = split_r_sans.state.astype(int)\n",
    "split_r_sans.action = split_r_sans.action.astype(int)\n",
    "split_r_sans.nstate = split_r_sans.nstate\n",
    "r_sans = r_sans.reset_index().merge(split_r_sans, left_index=True, right_index=True)\n",
    "r_sans.reward.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_r_sans = r_sans.merge(pr_trans, on='sans_id')\n",
    "assert pr_r_sans.pr_t.isna().sum() == 0, \"transition probabilities should never be na\"\n",
    "assert pr_r_sans.reward.isna().sum() == 0, \"rewards should never be na\"\n",
    "assert pr_r_sans.shape[0] == r_sans.shape[0], \"all reward rows should be in transition-reward dataframe\"\n",
    "assert pr_r_sans.shape[0] == pr_sans.shape[0], \"all transition rows should be in transition-reward dataframe\"\n",
    "assert pr_r_sans.sans_id.value_counts().max() == 1, \"a s-a-r-s tuple should occur exactly once\"\n",
    "assert pr_r_sans.sans_id.value_counts().min() == 1, \"a s-a-r-s tuple should occur exactly once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: element-wise multiply with transition matrix pr_trans\n",
    "pr_r_sans['weighted_reward'] = pr_r_sans.reward * pr_r_sans.pr_t\n",
    "immediate_reward = pr_r_sans.groupby(['state_x', 'action_x']).weighted_reward.sum()\n",
    "immediate_reward = immediate_reward.reset_index().rename(columns={'state_x': 'state', 'action_x': 'action', 'weighted_reward': 'immediate_reward'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immediate_reward.immediate_reward.hist()\n",
    "immediate_reward.immediate_reward.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immediate_reward['state_action_id'] = immediate_reward.agg('{0[state]:.0f}-{0[action]:.0f}'.format, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_trans[(pr_trans.state==0) & (pr_trans.action == 71)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_sans[pr_sans.sa_id=='0-71']['pr_sa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.000332 * train_set.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_nstate_state_action.sans_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_nstate_state_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_nstate_state_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_set.icustay_id == 299994).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shaping_name == 'unshaped':\n",
    "    test_set.to_csv('../mimic-data/test_{}_traj_{}.csv'.format(shaping_name, seed))\n",
    "    train_set.to_csv('../mimic-data/train_{}_traj_{}.csv'.format(shaping_name, seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety.action_compliance_clinical(train_set).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety.state_compliance_clinical(train_set, safety.avg_clinical_timestep).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immediate_reward[['state', 'action', 'immediate_reward']]\n",
    "# matrix of immediate rewards\n",
    "sar = np.full((n_states, 7**3), np.nan)\n",
    "for state, action, reward in immediate_reward[['state', 'action', 'immediate_reward']].to_numpy():\n",
    "    sar[int(state), int(action)] = reward\n",
    "\n",
    "# Qn = {} # maps n to q tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the tabular FQI algorithm in Ernst, Geurts & Wehenkel (2005), Figure 1\n",
    "# and Peine's supplementary discussion \"A: Evaluation of Policies\".\n",
    "def peine_mc_iterate(snsasr, r, Qn, gamma, n_epochs=1, learning_rate=0.1, unsafety_prob=0.0, safety_map=safety.action_id_compliance):\n",
    "    \"\"\"\n",
    "    Monte-carlo-based iteration of the training procedure according to tabular FQI & Peine's supplementary discussion.\n",
    "    \n",
    "    snsas: numpy ndarray with discretized state-nextstate-action tuples\n",
    "    r: a function that returns the immediate reward for a state-action pair\n",
    "    Qn: dictionary that maps iteration indices to Qn-estimates\n",
    "    n: iteration number\n",
    "    gamma: discount factor\n",
    "    n_epochs: number of times to iterate over dataset\n",
    "    learning rate: learning rate alpha\n",
    "    \"\"\"\n",
    "    def epoch(snsasr, r, Qn, gamma, learning_rate, unsafety_prob, safety_map):\n",
    "        for i, (s, ns, a, er) in enumerate(snsasr):\n",
    "            if unsafety_prob == 1.0:\n",
    "                # We do not care about the safety rules\n",
    "                Qn[s,a] = Qn[s,a] + learning_rate * (er + gamma * np.max(Qn[int(ns),:]) - Qn[s,a])\n",
    "            elif unsafety_prob == 0.0:\n",
    "                if safety_map[a]:\n",
    "                    Qn[s,a] = Qn[s,a] + learning_rate * (er + gamma * np.max(Qn[int(ns), safety_map]) - Qn[s,a])\n",
    "                else:\n",
    "                    # taken action not safe, disregard sample\n",
    "                    pass\n",
    "            else:\n",
    "                raise ValueError(\"Only unsafety probs in {0.0, 1.0} supported for now\")\n",
    "                #TODO FdH: implement unsafety probs (0.0, 0.0}\n",
    "        return Qn\n",
    "    assert Qn.shape == (n_states+2, 7**3)\n",
    "    assert safety_map is not None or unsafety_prob == 0.0\n",
    "    for n in range(n_epochs):\n",
    "        Qn = epoch(snsasr, r, Qn, gamma, learning_rate, unsafety_prob, safety_map)\n",
    "        assert np.nanmax(Qn) < 100, \"Scores > 100 should not occur, found: {}\".format(np.nanargmax(Qn))\n",
    "        print('.', end='')\n",
    "    return Qn\n",
    "\n",
    "\n",
    "def ernst_iterate(snsasr, Qn, n, gamma, regressor, action_space, state_repr=None, action_repr=utils.to_discrete_action_bins):\n",
    "    \"\"\"\n",
    "    snsas: numpy ndarray with discretized state_id-nextstate_id-action_id tuples\n",
    "    r: immediate rewards\n",
    "    Qn: dictionary that maps iteration indices to Qn estimates/estimators\n",
    "    n: iteration number\n",
    "    gamma: discount factor\n",
    "    regression: sklearn regression class\n",
    "    action_space: iterator that retuns all actions in\n",
    "    action_repr: function that returns the action representation\n",
    "    \"\"\"\n",
    "    def batches(X, batch_size):\n",
    "        n_batches = int(math.ceil(X.shape[0] / batch_size))\n",
    "        batches = []\n",
    "        for i in range(n_batches):\n",
    "            batch_start = i * batch_size\n",
    "            batch_end = min(batch_start + batch_size, X.shape[0])\n",
    "            batches.append((batch_start, batch_end))\n",
    "        return batches\n",
    "\n",
    "    def prediction_set(X, action_space):\n",
    "        shape_x = X.shape\n",
    "        n_actions = action_space.shape[0]\n",
    "        action_dims = action_space.shape[1]\n",
    "        repeated_X = X.repeat(n_actions, axis=0).reshape((shape_x[0] * n_actions, shape_x[1]))\n",
    "        repeated_action_reprs = np.tile(action_space, (shape_x[0], 1)).reshape(repeated_X.shape[0], action_dims)\n",
    "        return np.concatenate([repeated_X, repeated_action_reprs], axis=1)\n",
    "\n",
    "    Q_hat = Qn[n]\n",
    "    n = n + 1\n",
    "    # terminal states only\n",
    "    snsasr_t = snsasr[snsasr[:,1] >= 650] # 650 = number of non-terminal states\n",
    "    r_t = snsasr_t[:, 3]\n",
    "    # non-terminal states only\n",
    "    snsasr = snsasr[snsasr[:,1] < 650] # 650 = number of non-terminal states\n",
    "    a_reprs = np.array(list(map(action_repr, snsasr[:,2])))\n",
    "    action_space_reprs = np.array(list(map(action_repr, action_space)))\n",
    "    s_reprs = state_repr.transform(snsasr[:, 0].reshape(-1, 1))\n",
    "    ns_reprs = state_repr.transform(snsasr[:, 1].reshape(-1, 1))\n",
    "    r = snsasr[:, 3]\n",
    "    X_train = hstack((s_reprs, a_reprs))\n",
    "    # generate prediction dataset of length X.shape[0] * len(action_space)\n",
    "    batch_size = int(X_train.shape[0] / 100)\n",
    "    y_train = np.array([])\n",
    "    for batch_start, batch_end in batches(X_train, batch_size):\n",
    "        X_batch = ns_reprs[batch_start:batch_end, :].toarray()\n",
    "        X_preds = prediction_set(X_batch, action_space_reprs)\n",
    "        # predict Q scores for prediction dataset\n",
    "        y_preds = Q_hat.predict(X_preds)\n",
    "        # create targets with immediate reward and highest Q score for each row\n",
    "        y_preds = y_preds.reshape(X_batch.shape[0], len(action_space_reprs))\n",
    "        y_train = np.concatenate([y_train, r[batch_start:batch_end] + gamma * y_preds.max(axis=1)], axis=0)\n",
    "        print('.', end ='')\n",
    "    # terminal states\n",
    "    a_reprs_t = np.array(list(map(action_repr, snsasr_t[:,2])))\n",
    "    s_reprs_t = state_repr.transform(snsasr_t[:, 0].reshape(-1, 1))\n",
    "    X_train_t = hstack((s_reprs_t, a_reprs_t))\n",
    "    y_train_t = r_t\n",
    "    print(\"y_train_t {}\".format(y_train_t.shape))\n",
    "    X_train_n = vstack((X_train_t, X_train))\n",
    "    print(\"y_train {}\".format(y_train.shape))\n",
    "    y_train = np.concatenate((y_train, y_train_t))\n",
    "    print(\"y_train {}\".format(y_train.shape))\n",
    "    print(\"X_train_n {}\".format(X_train_n.shape))\n",
    "    Q_hat_new = regressor()\n",
    "    Q_hat_new.fit(X_train_n, y_train)\n",
    "    return n, Q_hat_new\n",
    "\n",
    "\n",
    "# TODO\n",
    "# implement a regressor that wraps an OLS regressor in the following way:\n",
    "#  1. creates an OLS regressor for each state\n",
    "#  2. implements a predict() function that first looks up the right state OLS\n",
    "#     and then calls predict() on the regressor there with the action levels as input\n",
    "#  3. implements a fit() function that first segments X into buckets with the same state-action pair\n",
    "#     then creates a regressor for that pair\n",
    "#     and calls fit() on that pair << TODO: BUT GIVEN WHAT INPUT? does this only work for a three-dimensional action input?\n",
    "class PerStateActionOLS():\n",
    "    def __init__(self, states, default=None):\n",
    "        self.regressors = {\n",
    "            state: LinearRegression() for state in states\n",
    "        }\n",
    "        self.default = default\n",
    "        \n",
    "    def fit(self, X, y, states):\n",
    "        \"\"\"\n",
    "        Fits a set of \n",
    "        \"\"\"\n",
    "        assert len(X) == len(states) == len(y), \"Number of train samples, labels and states should be equal\"\n",
    "        X = np.array(X)\n",
    "        X_df = pd.DataFrame(X, columns=['X_{}'.format(i) for i in range(X.shape[1])])\n",
    "        X_cols = list(X_df.columns)\n",
    "        X_df.loc[:, 'state'] = states\n",
    "        X_df.loc[:, 'y'] = y\n",
    "        groups = X_df.groupby('state')\n",
    "        for k in groups.groups.keys():\n",
    "            X_group = groups.get_group(k)[X_cols].to_numpy()\n",
    "            y_group = groups.get_group(k)['y'].to_numpy()\n",
    "            self.regressors[k].fit(X_group, y_group)\n",
    "    \n",
    "    def predict(self, X, states):\n",
    "        def predict_single(row, X_cols):\n",
    "            if row.state in self.regressors.keys():\n",
    "                return self.regressors[row.state].predict(row[X_cols].to_numpy().reshape(1,-1))\n",
    "            else:\n",
    "                if self.default is None:\n",
    "                    raise ValueError('Cannot predict for state {} without default prediction.'.format(row.state))\n",
    "                else:\n",
    "                    return self.default\n",
    "            \n",
    "        if isinstance(X, Iterable):\n",
    "            assert len(X) == len(states), \"Number of train samples, labels and states should be equal\"\n",
    "        X = np.array(X)\n",
    "        X_df = pd.DataFrame(X, columns=['X_{}'.format(i) for i in range(X.shape[1])])\n",
    "        X_cols = list(X_df.columns)\n",
    "        X_df.loc[:, 'state'] = states\n",
    "        predictions = X_df.apply(lambda x: predict_single(x, X_cols), axis=1)\n",
    "        return predictions\n",
    "\n",
    "class OneHotStateActionOLS():\n",
    "    def __init__(self, obs_states, all_states=range(650), *args, **kwargs):\n",
    "        # TODO: ensure that all states are passed\n",
    "        if type(obs_states) == set:\n",
    "            obs_states = list(obs_states)\n",
    "        obs_states = np.array(obs_states).reshape(-1, 1)\n",
    "        self.state_encoder = OneHotEncoder(categories=[all_states,]).fit(obs_states)\n",
    "        self.regressor = LinearRegression()\n",
    "    \n",
    "    def encode(self, X, states):\n",
    "        state_encoding = self.state_encoder.transform(states)\n",
    "        return np.concatenate((X, state_encoding.todense()), axis=1)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.regressor.fit(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        states = np.array(states).reshape(-1, 1)\n",
    "        X_new = self.encode(X, states)\n",
    "        return self.regressor.predict(X_new)\n",
    "\n",
    "\n",
    "class DummyRegressor:\n",
    "    def __init__(self, return_val=0):\n",
    "        self.return_val = return_val\n",
    "        \n",
    "    def predict(self, X, *args, **kwargs):\n",
    "        return np.repeat([self.return_val], X.shape[0]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_file = 'models/peine_mc_{}_{}_q_table_{}.bin'.format(shaping_name, UNSAFETY_PROB, seed)\n",
    "if os.path.isfile(q_file):\n",
    "    q_dumped = joblib.load(q_file)\n",
    "    q_mcp_nan = q_dumped['model']\n",
    "    q_mcp_loaded = True\n",
    "else:\n",
    "    q_mcp_loaded = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.01 * 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_training = True\n",
    "if not q_mcp_loaded or force_training:\n",
    "    q_init_val = 0\n",
    "    q_init = np.full((n_states + 2, 7**3), float(q_init_val))\n",
    "    #  peine_mc_iterate(snsas, r, Qn, gamma, n_epochs=1, learning_rate=0.1):\n",
    "    q_mcp = peine_mc_iterate(\n",
    "        # TODO: why are the NaNs here? how to deal with these?\n",
    "        snsasr=train_set[['state', 'next_state', 'action_discrete', 'reward']].astype(int).to_numpy(),\n",
    "        r=sar,\n",
    "        Qn=q_init,\n",
    "        gamma=GAMMA,\n",
    "        n_epochs=10000,\n",
    "        learning_rate=0.01,\n",
    "        unsafety_prob=UNSAFETY_PROB,\n",
    "        safety_map=safety.action_id_compliance\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not q_mcp_loaded or force_training:\n",
    "    q_mcp_nan = q_mcp.copy()[:n_states, :]\n",
    "    q_mcp_nan[q_mcp_nan == 0.0] = np.nan\n",
    "else:\n",
    "    q_mcp = np.nan_to_num(q_mcp_nan, 0.0)\n",
    "sns.histplot(q_mcp.ravel(), bins=200)\n",
    "plt.xlabel('Q value')\n",
    "plt.title('Histogram of all Q values in Q table')\n",
    "plt.show()\n",
    "sns.histplot(q_mcp_nan.ravel(), bins=200)\n",
    "plt.xlabel('Q value')\n",
    "plt.title('Histogram of nonzero Q values in Q table')\n",
    "plt.show()\n",
    "sns.histplot(q_mcp_nan.ravel(), log_scale=(False, True), bins=200)\n",
    "plt.xlabel('Q value')\n",
    "plt.title('Histogram of nonzero Q values in Q table, logscale')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not q_mcp_loaded:\n",
    "    joblib.dump(\n",
    "        {'hyperparameters': {\n",
    "            'Q_init': q_init,\n",
    "            'gamma': GAMMA,\n",
    "            'n_epochs': 10000,\n",
    "            'learning_rate': 0.01,\n",
    "            'shaping': SHAPING,\n",
    "            'shaping_scalar': COMPLIANCE_SCALAR,\n",
    "            'unsafety_prob':UNSAFETY_PROB,\n",
    "            'safety_map':safety.action_id_compliance            \n",
    "        },\n",
    "        'model': q_mcp_nan,\n",
    "        },\n",
    "        'models/peine_mc_{}_{}_q_table_{}.bin'.format(shaping_name, UNSAFETY_PROB, seed),\n",
    "        compress=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive policy by taking argmax over non-nan q values\n",
    "# policy_mcp = np.full()\n",
    "q_mcp_nan[np.isnan(q_mcp_nan).all(axis=1),:] = 0\n",
    "\n",
    "best_action_indices = np.nanargmax(q_mcp_nan, axis=1)\n",
    "\n",
    "action_index_grid = np.tile(np.array(range(7**3)), 650).reshape((650, 7**3))\n",
    "best_action_grid = np.repeat(best_action_indices, 7**3).reshape((650, 7**3))\n",
    "best_action_bool = best_action_grid == action_index_grid\n",
    "assert best_action_bool.shape == (n_states, 7**3)\n",
    "assert (best_action_bool.sum(axis=1) == 1).all()\n",
    "mcp_greedy = best_action_bool.astype(float)\n",
    "assert (mcp_greedy.sum(axis=1) == 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive policy by taking softmax\n",
    "q_mcp_neg = q_mcp.copy()[:n_states, :]\n",
    "q_mcp_neg[q_mcp_neg == 0.0] = float('-inf')\n",
    "mcp_softmax = scipy.special.softmax(q_mcp_neg, axis=1)\n",
    "assert mcp_softmax.shape == (n_states, 7**3)\n",
    "assert (mcp_greedy.sum(axis=1) == 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_s, best_a = np.unravel_index(np.nanargmax(q_mcp_nan), (n_states, 7**3))\n",
    "print(\"Global highest Q value {} for tv, fio2, peep ranges: {}\".format(q_mcp_nan[best_s, best_a], utils.to_action_ranges(best_a)))\n",
    "best_mean_a, best_mean_a_q = np.nanargmax(np.nanmean(q_mcp_nan, axis=0)), np.nanmax(np.nanmean(q_mcp_nan, axis=0))\n",
    "print(\"Highest avg Q value across states {} for tv, fio2, peep ranges: {}\".format(best_mean_a_q, utils.to_action_ranges(best_mean_a)))\n",
    "best_med_a, best_med_a_q = np.nanargmax(np.nanmedian(q_mcp_nan, axis=0)), np.nanmax(np.nanmedian(q_mcp_nan, axis=0))\n",
    "print(\"Highest median Q value across states {} for tv, fio2, peep ranges: {}\".format(best_med_a, utils.to_action_ranges(best_med_a)))\n",
    "sns.histplot(q_mcp_nan[mcp_greedy == 1.0].ravel(), log_scale=(False, True), bins=200)\n",
    "plt.xlabel('Q value')\n",
    "plt.title('Histogram of Q values greedy policy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['positive_outcome'] = train_set['mort90day'] == 'f'\n",
    "estimated_mort_state_visit = train_set.groupby('state').mean('positive_outcome')[['positive_outcome']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=np.nanmean(q_mcp_nan, axis=1), y=estimated_mort_state_visit.reshape(n_states,))\n",
    "plt.xlabel('Mean estimated Q value')\n",
    "plt.ylabel('Average outcome')\n",
    "plt.title('Outcome vs mean Q value estimates')\n",
    "plt.show()\n",
    "sns.scatterplot(x=np.nanmax(q_mcp_nan, axis=1), y=estimated_mort_state_visit.reshape(n_states,))\n",
    "plt.xlabel('Max estimated Q value')\n",
    "plt.ylabel('Average outcome')\n",
    "plt.title('Outcome vs max Q value estimates')\n",
    "plt.show()\n",
    "sns.scatterplot(x=np.nanmedian(q_mcp_nan, axis=1), y=estimated_mort_state_visit.reshape(n_states,))\n",
    "plt.xlabel('Max estimated Q value')\n",
    "plt.ylabel('Average outcome')\n",
    "plt.title('Outcome vs median Q value estimates')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_vars = np.nanvar(q_mcp_nan, axis=1)\n",
    "q_means = np.nanmean(q_mcp_nan, axis=1)\n",
    "q_medians = np.nanmedian(q_mcp_nan, axis=1)\n",
    "q_maxs = np.nanmax(q_mcp_nan, axis=1)\n",
    "q_mins = np.nanmin(q_mcp_nan, axis=1)\n",
    "stacked = np.column_stack((q_means, q_medians, q_maxs, q_mins, q_vars))\n",
    "xs = range(n_states)\n",
    "means_sorted = np.array(sorted(stacked, key=lambda x: x[0]))\n",
    "means_upper = means_sorted[:, 0] + means_sorted[:, -1]\n",
    "means_lower = means_sorted[:, 0] - means_sorted[:, -1]\n",
    "axs = sns.lineplot(x=xs, y=means_sorted[:, 0])\n",
    "axs.fill_between(x=xs, y1=means_lower, y2=means_upper, alpha=.3)\n",
    "axs.set_ylim(-150, 150)\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Q value')\n",
    "plt.title('Mean Q values per state +- 1 var')\n",
    "plt.show()\n",
    "\n",
    "medians_sorted = np.array(sorted(stacked, key=lambda x: x[1]))\n",
    "means_upper = medians_sorted[:, 0] + medians_sorted[:, -1]\n",
    "means_lower = medians_sorted[:, 0] - medians_sorted[:, -1]\n",
    "axs = sns.lineplot(x=xs, y=medians_sorted[:, 1])\n",
    "axs.fill_between(x=xs, y1=means_lower, y2=means_upper, alpha=.3)\n",
    "axs.set_ylim(-150, 150)\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Q value')\n",
    "plt.title('Median Q values per state +- 1 var')\n",
    "plt.show()\n",
    "\n",
    "mins_sorted = np.array(sorted(stacked, key=lambda x: x[3]))\n",
    "axs = sns.scatterplot(x=xs, y=mins_sorted[:, 3], color='orange', alpha=.5)\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Q value')\n",
    "plt.title('Min Q values per state')\n",
    "\n",
    "maxs_sorted = np.array(sorted(stacked, key=lambda x: x[2]))\n",
    "axs = sns.lineplot(x=xs, y=maxs_sorted[:, 2], alpha=.5)\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Q value')\n",
    "plt.title('Max Q values per state')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortnan(x, index):\n",
    "    return float('-inf') if np.isnan(x[index]) else x[index]\n",
    "\n",
    "q_vars = np.nanvar(q_mcp_nan, axis=0)\n",
    "q_means = np.nanmean(q_mcp_nan, axis=0)\n",
    "q_medians = np.nanmedian(q_mcp_nan, axis=0)\n",
    "q_maxs = np.nanmax(q_mcp_nan, axis=0)\n",
    "q_mins = np.nanmin(q_mcp_nan, axis=0)\n",
    "stacked = np.column_stack((q_means, q_medians, q_maxs, q_mins, q_vars))\n",
    "xs = range(7**3)\n",
    "means_sorted = np.array(sorted(stacked, key=lambda x: sortnan(x, 0)))\n",
    "means_upper = means_sorted[:, 0] + means_sorted[:, -1]\n",
    "means_lower = means_sorted[:, 0] - means_sorted[:, -1]\n",
    "axs = sns.lineplot(x=xs, y=means_sorted[:, 0])\n",
    "axs.fill_between(x=xs, y1=means_lower, y2=means_upper, alpha=.3)\n",
    "axs.set_ylim(-150, 150)\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Q value')\n",
    "plt.title('Mean Q values per action +- 1 var')\n",
    "plt.show()\n",
    "\n",
    "medians_sorted = np.array(sorted(stacked, key=lambda x: sortnan(x, 1)))\n",
    "means_upper = medians_sorted[:, 0] + medians_sorted[:, -1]\n",
    "means_lower = medians_sorted[:, 0] - medians_sorted[:, -1]\n",
    "axs = sns.lineplot(x=xs, y=medians_sorted[:, 1])\n",
    "axs.fill_between(x=xs, y1=means_lower, y2=means_upper, alpha=.3)\n",
    "axs.set_ylim(-150, 150)\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Q value')\n",
    "plt.title('Median Q values per action +- 1 var')\n",
    "plt.show()\n",
    "\n",
    "maxs_sorted = np.array(sorted(stacked, key=lambda x: sortnan(x, 2)))\n",
    "axs = sns.lineplot(x=xs, y=maxs_sorted[:, 2])\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Q value')\n",
    "plt.title('Max Q values per action')\n",
    "plt.show()\n",
    "\n",
    "mins_sorted = np.array(sorted(stacked, key=lambda x: sortnan(x, 3)))\n",
    "axs = sns.lineplot(x=xs, y=mins_sorted[:, 3])\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Q value')\n",
    "plt.title('Min Q values per action')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_policy_df = (test_set.value_counts(['state', 'action_discrete']) / test_set.value_counts(['state']))\n",
    "assert (1.0 - behavior_policy_df.groupby('state').sum() < 1e10).all(), \"Policy action probs should sum to 1 per state\"\n",
    "behavior_policy_df = behavior_policy_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_policy_df = train_set.value_counts(['state', 'action_discrete']) / train_set.value_counts(['state'])\n",
    "assert (1.0 - behavior_policy_df.groupby('state').sum() < 1e10).all(), \"Policy action probs should sum to 1 per state\"\n",
    "\n",
    "behavior_policy_pivot = behavior_policy_df.reset_index().pivot(columns='action_discrete', index='state')[0]\n",
    "behavior_policy_states = set(behavior_policy_pivot.index.unique())\n",
    "for s in range(n_states):\n",
    "    if s not in behavior_policy_states:\n",
    "        action_probs = [1.0 / (7**3),] * 7**3 # uniform distribution\n",
    "        for i, p in enumerate(action_probs):\n",
    "            behavior_policy_pivot.loc[s] = [s, i, p]\n",
    "\n",
    "behavior_policy_pivot = behavior_policy_pivot.sort_values(['state'])\n",
    "\n",
    "for a in range(7**3):\n",
    "    if a not in behavior_policy_pivot.columns:\n",
    "        behavior_policy_pivot.loc[:, a] = np.nan\n",
    "\n",
    "behavior_policy_nan = behavior_policy_pivot[range(7**3)].to_numpy()\n",
    "assert (1- (np.nansum(behavior_policy_nan, axis=1)) < 1e10).all(), \"Policy action probs should sum to 1 per state\"\n",
    "behavior_policy = np.nan_to_num(behavior_policy_nan, 0.0)\n",
    "assert (1- (behavior_policy.sum(axis=1)) < 1e10).all(), \"Policy action probs should sum to 1 per state\"\n",
    "assert behavior_policy.shape == (n_states, 7**3), \"Behavior policy should cover all states and actions\"\n",
    "mcp_greedy_mask = mcp_greedy.astype(bool)\n",
    "assert (mcp_greedy_mask.sum(axis=1) == 1).all(), \"Greedy policy mask should mask out all-but-one action\"\n",
    "if UNSAFETY_PROB == 1.0:\n",
    "    joblib.dump(behavior_policy, \"models/clinicians_policy_train_{}.bin\".format(seed), compress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_policy_df = test_set.value_counts(['state', 'action_discrete']) / test_set.value_counts(['state'])\n",
    "assert (1.0 - behavior_policy_df.groupby('state').sum() < 1e10).all(), \"Policy action probs should sum to 1 per state\"\n",
    "\n",
    "behavior_policy_pivot = behavior_policy_df.reset_index().pivot(columns='action_discrete', index='state')[0]\n",
    "\n",
    "for a in range(7**3):\n",
    "    if a not in behavior_policy_pivot.columns:\n",
    "        behavior_policy_pivot.loc[:, a] = np.nan\n",
    "behavior_policy_states = set(behavior_policy_pivot.index.unique())\n",
    "for s in range(n_states):\n",
    "    if s not in behavior_policy_states:\n",
    "        action_probs = [1.0 / (7**3),] * 7**3 # uniform distribution\n",
    "        for i, p in enumerate(action_probs):\n",
    "            behavior_policy_pivot.loc[s] = [1/(7**3),]*(7**3)\n",
    "\n",
    "behavior_policy_pivot = behavior_policy_pivot.sort_values(['state'])\n",
    "\n",
    "behavior_policy_nan = behavior_policy_pivot[range(7**3)].to_numpy()\n",
    "assert (1- (np.nansum(behavior_policy_nan, axis=1)) < 1e10).all(), \"Policy action probs should sum to 1 per state\"\n",
    "behavior_policy = np.nan_to_num(behavior_policy_nan, 0.0)\n",
    "assert (1- (behavior_policy.sum(axis=1)) < 1e10).all(), \"Policy action probs should sum to 1 per state\"\n",
    "assert behavior_policy.shape == (n_states, 7**3), \"Behavior policy should cover all states and actions\"\n",
    "mcp_greedy_mask = mcp_greedy.astype(bool)\n",
    "assert (mcp_greedy_mask.sum(axis=1) == 1).all(), \"Greedy policy mask should mask out all-but-one action\"\n",
    "if UNSAFETY_PROB == 1.0:\n",
    "    joblib.dump(behavior_policy, \"models/clinicians_policy_test_{}.bin\".format(seed), compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = pd.concat([train_set, test_set])\n",
    "behavior_policy_df = train_test.value_counts(['state', 'action_discrete']) / train_test.value_counts(['state'])\n",
    "assert (1.0 - behavior_policy_df.groupby('state').sum() < 1e10).all(), \"Policy action probs should sum to 1 per state\"\n",
    "\n",
    "behavior_policy_pivot = behavior_policy_df.reset_index().pivot(columns='action_discrete', index='state')[0]\n",
    "for a in range(7**3):\n",
    "    if a not in behavior_policy_pivot.columns:\n",
    "        behavior_policy_pivot.loc[:, a] = np.nan\n",
    "\n",
    "behavior_policy_nan = behavior_policy_pivot[range(7**3)].to_numpy()\n",
    "assert (1- (np.nansum(behavior_policy_nan, axis=1)) < 1e10).all(), \"Policy action probs should sum to 1 per state\"\n",
    "behavior_policy = np.nan_to_num(behavior_policy_nan, 0.0)\n",
    "assert (1- (behavior_policy.sum(axis=1)) < 1e10).all(), \"Policy action probs should sum to 1 per state\"\n",
    "assert behavior_policy.shape == (n_states, 7**3), \"Behavior policy should cover all states and actions\"\n",
    "mcp_greedy_mask = mcp_greedy.astype(bool)\n",
    "assert (mcp_greedy_mask.sum(axis=1) == 1).all(), \"Greedy policy mask should mask out all-but-one action\"\n",
    "if UNSAFETY_PROB == 1.0:\n",
    "    joblib.dump(behavior_policy, \"models/clinicians_policy_train_test_{}.bin\".format(seed), compress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "scipy.stats.entropy(behavior_policy.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(scipy.stats.entropy(behavior_policy, axis=1))\n",
    "plt.title('Behavior policy per-state entropy')\n",
    "plt.xlabel('Entropy') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(scipy.stats.entropy(mcp_softmax, axis=1))\n",
    "plt.title('Softmax policy per-state entropy')\n",
    "plt.xlabel('Entropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Behavior policy argmax and greedy policy agreement: {}\".format((behavior_policy.argmax(axis=1) == mcp_greedy.argmax(axis=1)).sum() / n_states))\n",
    "sns.histplot(behavior_policy[mcp_greedy_mask], log_scale=(False, True))\n",
    "plt.xlabel('Action probability greedy policy in behavior policy')\n",
    "behavior_policy[mcp_greedy_mask].min(), behavior_policy[mcp_greedy_mask].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_policy = mcp_greedy\n",
    "\n",
    "behavior_policy_ranks = np.flip(behavior_policy.argsort(axis=1), axis=1)\n",
    "ep_bp_ranks = []\n",
    "for s in range(n_states):\n",
    "    ep_a = evaluation_policy[s,:].argmax()\n",
    "    bp_rank = np.where(behavior_policy_ranks[s, :] == ep_a)[0][0]\n",
    "    ep_bp_ranks.append(bp_rank)\n",
    "\n",
    "sns.histplot(ep_bp_ranks, bins=60)\n",
    "plt.title('Greedy policy action ranks in behavior policy')\n",
    "plt.xlabel('Rank')\n",
    "plt.show()\n",
    "\n",
    "behavior_policy_ranked_probs = np.flip(np.sort(behavior_policy, axis=1), axis=1)\n",
    "ep_bp_prob_mass = []\n",
    "for s in range(n_states):\n",
    "    ep_a = evaluation_policy[s,:].argmax()\n",
    "    bp_rank = np.where(behavior_policy_ranks[s, :] == ep_a)[0][0]\n",
    "    ep_bp_prob_mass.append(behavior_policy_ranked_probs[s, 0:bp_rank].sum())\n",
    "\n",
    "sns.histplot(ep_bp_prob_mass)\n",
    "plt.title('Probability mass up to greedy actduion')\n",
    "plt.xlabel('Action probs')\n",
    "plt.show()\n",
    "np.array(ep_bp_prob_mass).min(), np.array(ep_bp_prob_mass).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state-values of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(mcp_greedy, \"models/mcp_greedy_policy_{}_{}_{}.bin\".format(seed, shaping_name, UNSAFETY_PROB), compress=True)\n",
    "joblib.dump(mcp_softmax, \"models/mcp_softmax_policy_{}_{}_{}.bin\".format(seed, shaping_name, UNSAFETY_PROB), compress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(safety)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
